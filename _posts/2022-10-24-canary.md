---
layout: post
title: "[Infra] Kubernetes Canary 배포 전략 실습"
date: 2022-10-24
desc: "[Infra] Kubernetes Canary 배포 전략 실습"
keywords: "infra,docker,minikube,kubernetes,grafana,canary"
categories: [Infra]
tags: [infra,docker,minikube,kubernetes,grafana,canary]
icon: icon-html
---

본 포스팅에서는 Kubernetes의 여러 배포 전략 중 Canary에 대해 알아보고 실습해볼 것이다. 실습 진행은 [해당 깃허브 내용](https://github.com/ContainerSolutions/k8s-deployment-strategies)을 참고하였다.

---

## 목차

[1. Canary란?](#list1)

[2. Canary 실습](#list2)

[&nbsp;&nbsp; 2.1. 실습 환경 구성](#list3_1)

[&nbsp;&nbsp; 2.2. Canary 앱 배포](#list3_2)

[3. Grafana dashboard 확인](#list3)

---

## **1. Canary란?** <a name="list1"></a>

  Canary 배포는 특정 서버나 소수의 유저들에게만 새로운 버전을 배포하여 테스트하면서 추후 안전하다는 판단이 되면 모든 서버들에 새로운 버전을 배포하는 방식이다. 배포 방식을 그림으로 살펴보자면 다음 [그림 1]과 같다.

  | ![canary](/static/assets/img/landing/infra/canary1.png){: width="100%"} |
  |:--:| 
  | [그림 1] Canary 배포 과정 |

  <br>

  이 기법은 Blue/Green 배포 방식과 유사하게 라벨을 이용하고, A/B 테스트 방식과 성능 모니터링에 유용하다고 한다.

  <br>

## **2. Canary 실습** <a name="list2"></a>

<br>

### **2.1. 실습 환경 구성** <a name="list2_1"></a>

  실습은 이전 포스팅에서 진행했던 blue-green/single-service 폴더에서 canary/native 폴더로 이동하여 진행한다.

  ```bash
  son@son-localhost ~/k8s-deployment-strategies/blue-green/single-service $ cd ../../canary/native
  son@son-localhost ~/k8s-deployment-strategies/canary/native $ cat app-v1.yaml
  apiVersion: v1
  kind: Service
  metadata:
    name: my-app
    labels:
      app: my-app
  spec:
    type: NodePort
    ports:
    - name: http
      port: 80
      targetPort: http
    selector:
      app: my-app
  ---
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: my-app-v1
    labels:
      app: my-app
  spec:
    replicas: 10
    selector:
      matchLabels:
        app: my-app
        version: v1.0.0
    template:
      metadata:
        labels:
          app: my-app
          version: v1.0.0
        annotations:
          prometheus.io/scrape: "true"
          prometheus.io/port: "9101"
      spec:
        containers:
        - name: my-app
          image: crisis513/strategies-test
          ports:
          - name: http
            containerPort: 8080
          - name: probe
            containerPort: 8086
          env:
          - name: VERSION
            value: v1.0.0
          livenessProbe:
            httpGet:
              path: /live
              port: probe
            initialDelaySeconds: 5
            periodSeconds: 5
          readinessProbe:
            httpGet:
              path: /ready
              port: probe
            periodSeconds: 5
  son@son-localhost ~/k8s-deployment-strategies/canary/native $ cat app-v2.yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: my-app-v2
    labels:
      app: my-app
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: my-app
        version: v2.0.0
    template:
      metadata:
        labels:
          app: my-app
          version: v2.0.0
        annotations:
          prometheus.io/scrape: "true"
          prometheus.io/port: "9101"
      spec:
        containers:
        - name: my-app
          image: crisis513/strategies-test
          ports:
          - name: http
            containerPort: 8080
          - name: probe
            containerPort: 8086
          env:
          - name: VERSION
            value: v2.0.0
          livenessProbe:
            httpGet:
              path: /live
              port: probe
            initialDelaySeconds: 5
            periodSeconds: 5
          readinessProbe:
            httpGet:
              path: /ready
              port: probe
            periodSeconds: 5
  ```

  매니페스트를 살펴보면 Blue/Green과 크게 다를 것이 없다. 다른 점은 replicas의 갯수 차이이다. app-v1.yaml에서는 replicas가 10으로 설정되어 있고, app-v2.yaml에는 1로 설정되어 있는 것을 확인할 수 있다.

  <br>

### **2.2. Canary 앱 배포** <a name="list2_2"></a>

  먼저 app-v1.yaml을 배포해보자.

  ```bash
  son@son-localhost ~/k8s-deployment-strategies/canary/native $ k apply -f app-v1.yaml
  service/my-app created
  deployment.apps/my-app-v1 created

  son@son-localhost ~/k8s-deployment-strategies/canary/native $ k get all
  NAME                            READY   STATUS    RESTARTS   AGE
  pod/my-app-v1-9b99b584b-5kc4w   1/1     Running   0          114s
  pod/my-app-v1-9b99b584b-5ksm4   1/1     Running   0          113s
  pod/my-app-v1-9b99b584b-8gdzk   1/1     Running   0          113s
  pod/my-app-v1-9b99b584b-bxlsx   1/1     Running   0          113s
  pod/my-app-v1-9b99b584b-fvmwf   1/1     Running   0          114s
  pod/my-app-v1-9b99b584b-ng5jn   1/1     Running   0          114s
  pod/my-app-v1-9b99b584b-qqm2f   1/1     Running   0          113s
  pod/my-app-v1-9b99b584b-sqhsf   1/1     Running   0          113s
  pod/my-app-v1-9b99b584b-wlz4n   1/1     Running   0          113s
  pod/my-app-v1-9b99b584b-xxkd5   1/1     Running   0          113s

  NAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
  service/kubernetes   ClusterIP   10.96.0.1      <none>        443/TCP        15d
  service/my-app       NodePort    10.103.42.11   <none>        80:32631/TCP   117s

  NAME                        READY   UP-TO-DATE   AVAILABLE   AGE
  deployment.apps/my-app-v1   10/10   10           10          116s

  NAME                                  DESIRED   CURRENT   READY   AGE
  replicaset.apps/my-app-v1-9b99b584b   10        10        10      115s
  ```

  서비스가 올라가고 pod 10개가 정상적으로 만들어졌다면 이전 포스팅의 배포 전략들과 마찬가지로 메트릭을 생성해내기 위해 해당 애플리케이션으로 지속적인 요청을 보내보도록 하겠다.

  ```bash
  son@son-localhost ~/k8s-deployment-strategies/canary/native $ while sleep 0.1; do curl -s -o /dev/null "http://192.168.49.2:32631"; done
  ```

  마찬가지로 요청은 계속 보내도록 두고 약간의 시간이 지나고나서 새로운 터미널을 열어 app-v2.yaml을 배포할 것이다. 배포를 하고나면 my-app-v2 pod가 하나 생성되기 때문에 my-app-v1의 replicas를 하나 줄인 9개로 설정할 것이다.


  ```bash
  son@son-localhost ~/k8s-deployment-strategies/canary/native $ k apply -f app-v2.yaml
  deployment.apps/my-app-v2 created
  son@son-localhost ~/k8s-deployment-strategies/canary/native $ k scale --replicas=9 deploy my-app-v1
  deployment.apps/my-app-v1 scaled
  ```
  
  여기까지 진행됐다면 현재 9개의 my-app-v1 pod가 9개, my-app-v2 pod가 1개로 Running 중일 것이다. 즉, 전체 트래픽의 90%는 기존 버전의 애플리케이션으로, 나머지 10%는 새로운 버전의 애플리케이션으로 이동하게 될 것이다. curl을 통한 요청이 지속 되고 있으므로 이 상태로도 일정 시간 지나고나서 my-app-v2의 replicas를 10개로 늘리고 기존 버전인 my-app-v1을 모두 제거할 것이다. 

  ```bash
  son@son-localhost ~/k8s-deployment-strategies/canary/native $ k scale --replicas=10 deploy my-app-v2
  deployment.apps/my-app-v2 scaled
  son@son-localhost ~/k8s-deployment-strategies/canary/native $ k delete deploy my-app-v1
  deployment.apps "my-app-v1" deleted
  ```

  <br>

  다음 영상은 app-v1.yaml 파일이 배포되어 있는 상태에서 위의 app-v2.yaml을 배포하는 과정을 촬영한 것이다. 위에서 설명한 명령마다 pod가 어떻게 변화하는지를 확인할 수 있다.

  <video width="100%" preload="auto" muted controls>
      <source src="/static/assets/video/blog/canary.mp4" type="video/mp4" />
  </video>

  <br>

## 3. **Grafana dashboard 확인** <a name="list3"></a>

  Canary 배포를 통해 정상적으로 새로운 버전으로 배포 되었다면 마찬가지로 Grafana를 통해 Canary 배포 과정에 대한 지속적인 요청 메트릭을 확인해볼 것이다. 이전 포스팅에서 배포해 두었던 Grafana에 접속하기 위해 localhost:3000으로 접근하여 만들어 둔 패널을 **refresh**해보자. 다음 [그림 2]와 같은 화면을 볼 수 있을 것이다. 

  | ![request_total_panel](/static/assets/img/landing/infra/canary2.png){: width="100%"} |
  |:--:| 
  | [그림 2] Request Total 패널 확인 |

  <br>
  
  my-app-v2가 배포되었을 때는 약 10%의 메트릭이 점진적으로 발생한 것을 볼 수 있고, 그 후에 my-app-v2의 replicas를 10으로 올리고, my-app-v1을 삭제하고나서부터는 점진적으로 my-app-v2로 교체되는 것을 확인할 수 있다. Blue/Green과 매니페스트는 유사하지만 이렇게 가능한 이유는 위의 두 버전의 deployment는 동일한 app selector를 가지기 때문이다. 

  <br>

  여기까지 Kubernetes에서 무중단 배포 전략들의 구현 방법과 Grafana를 통한 배포 과정에 대한 메트릭을 확인해보는 시간을 가져보았다. 우리가 운영하는 서비스의 특징 혹은 배포할 때 필요한 요구사항들을 잘 고려하여 어떤 전략으로 배포할지 잘 결정해야할 것 같다.

  <br>